{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hi</p> <p>You have reached my personal site.</p> <p>Friends and colleagues know me as one or more of the following.</p> <ul> <li>Sriramkrishnan Srinivasan</li> <li>Sriram Srinivasan</li> <li>Sriram</li> <li>Sri</li> <li>(Sri)^3</li> <li>shrichris</li> </ul> <p>Since Feb 2023, I have been using this site to document notes to help me structure learning and act as a memory aid.</p>"},{"location":"#elsewhere-on-the-interwebs","title":"Elsewhere on the interwebs:","text":"<ul> <li>2006-2010</li> <li>2010-2012</li> <li>2012 -   </li> <li>2023 -   </li> </ul>"},{"location":"cheatsheets/2023-02-04-Git-Notes/","title":"Git Notes","text":"","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#set-and-retrieve-username-and-email","title":"Set and retrieve username and email:","text":"<pre><code>git config --global user.name \"your name\"\ngit config --global user.email \"your@email.com\" </code></pre> <p>Check using</p> <pre><code>git config user.name\ngit config user.email\n</code></pre>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#see-git-status","title":"See git status","text":"<pre><code>git status\n</code></pre>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#viewing-git-logs-in-the-terminal","title":"Viewing git logs in the terminal","text":"<p><pre><code>git log </code></pre> Shows a log of all commits</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#tweaks-for-concise-log-output","title":"Tweaks for concise log output","text":"<p><pre><code>git log \u2014pretty=oneline \u2014abbrev-commit\n</code></pre> or <pre><code>git log \u2014oneline \u2014abbrev-commit\n</code></pre></p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#create-a-git-repo","title":"Create a git repo","text":"<pre><code>git init\n</code></pre> <p>git tracks nested folders. Do not initialise a repo inside another repo. You can check this by running git status inside the folder you want to track before running git init.</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#staging-changes","title":"Staging changes","text":"<p>Commits should ideally be be grouped/staged with</p> <pre><code>git add &lt;file1&gt; &lt;file2&gt;\n</code></pre> <p>To stage all changes, use <pre><code>git add .\n</code></pre></p> <p><code>git add</code> tells git to start tracking  * newly added files * files that were not previously tracked (or deleted files?) * files that are modified! in the working folder.</p> <p>This puts the files in a \u201cstaging area\u201d</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#committing-to-a-repo","title":"Committing to a repo","text":"<p>A commit is  * not the same as a save on an individual file * can be thought of as a collection of changes that have been saved</p> <p>The .git folder is the actual repository. A git commit changes the .git folder <pre><code>git commit -m &lt;commit message summarising changes&gt;\n</code></pre></p> <p>Use this -m flag to avoid vi jail hell :)</p> <p>After a commit (assuming all changes to tracked files are committed), git status will return a \u201cworking tree clean, nothing to commit\u201d message</p> <p>N.B. To both stage and commit, use</p> <pre><code>git commit -a -m &lt;commit message&gt;\n</code></pre>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#commit-message-best-practice","title":"Commit Message Best Practice?:","text":"<p>Git recommends using \u201cpresent tense imperative style/mood\u201d for commit messages</p> <p>e.g. make a do b, rather than \u201cThis change makes\u2026\u201d, or \u201c I made\u2026\u201d</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#atomic-git-commits","title":"Atomic git commits","text":"<p>The idea that git commits should focus on a single feature, fix or change</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#gitignore","title":".gitignore","text":"<p>Always exclude  Secrets/API keys etc .DS_Store on Mac Log files Dependencies and packages (node, python modules etc) - can be rebuilt easily </p> <p>N.B to exclude a directory, remember to include then/ at the end of the directory name. Without the / git will think it is a file.</p> <p>Remember to track the gitignore file :)</p> <p>gitignore.io is a repository of representative gitignore templates.</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#to-make-a-change-to-the-most-recent-commit","title":"To make a change to the most recent commit","text":"<p>make additional change e.g. git add &lt;&gt; git commit \u2014amend</p> <p>Only works for most recent commit</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#list-branches-of-a-repo","title":"List branches of a repo","text":"<pre><code>git branch\n</code></pre> <p>The active branch is highlighted with a *</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#create-a-branch","title":"Create a branch","text":"<pre><code>git branch &lt;newbranchname&gt;\n</code></pre> <p>This command creates a new branch based on the current HEAD. In other words, the new branch points to the same location (technically the same commit) as the current HEAD </p> <p>Creating a branch does not switch you that branch </p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#switching-to-a-branch","title":"Switching to a branch","text":"<pre><code>git switch &lt;branch name&gt; \n</code></pre> <p>or use the older style less preferred equivalent </p> <p><pre><code>git checkout &lt;branch name&gt; \n</code></pre> git checkout is also used to restore working tree files</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#create-and-switch-to-a-new-branch-in-one-step","title":"Create and switch to a new branch in one step","text":"<p><pre><code>git switch -c &lt;branch name&gt; \n</code></pre> or <pre><code>git checkout -b &lt;branch name&gt;\n</code></pre></p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#branch-switching-nuances","title":"Branch switching nuances","text":"","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#nuance-1","title":"Nuance 1","text":"<ul> <li>If<ul> <li>there are files that only exist in the current branch and we want to switch to another branch</li> </ul> </li> <li>Then<ul> <li>the new files will be carried over to the branch we switch to</li> </ul> </li> <li>Because<ul> <li>There will be no conflict between the branches</li> </ul> </li> </ul>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#nuance-2","title":"Nuance 2","text":"<ul> <li>If<ul> <li>there are files that exist in both the current branch and in a branch we want to switch to</li> </ul> </li> <li>Then<ul> <li>Changes in the current branch must be commited (or stashed?) before switching to a new branch. </li> </ul> </li> <li>Else<ul> <li>Any uncommited changes will be LOST!</li> </ul> </li> <li>Because<ul> <li>The files will be in conflict between the two branches</li> </ul> </li> </ul>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#deleting-a-branch","title":"Deleting a branch","text":"<pre><code>git branch -d &lt;branchname&gt;\n</code></pre> <p>This command requires that  * we are not on the currently checked out branch i.e. HEAD must not be pointing at  * changes in the branches we want to delete are fully merged <p>Deletion can be forced with -D flag</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#renaming-a-branch","title":"Renaming a branch","text":"<p>We will need to do this when using github, to rename the \u201cmaster\u201d branch in git land to \u201cmain\u201d branch in github land</p> <pre><code>git branch -m &lt;new name&gt;\n</code></pre> <p>This command requires that we are IN (or On?) the branch we want to rename!</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#what-is-head","title":"What is HEAD?","text":"<ul> <li>Think of Head as an analogue of a tape head. It points to what is current location that is being read. So if HEAD is pointing to master, this means that the master branch is being read.</li> </ul>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#what-is-detached-head","title":"What is detached head?","text":"<ul> <li>Detached HEAD is when head is not pointing to a specific branch but to a specific commit.</li> <li>Useful for example to check the status of a repo at the point that a specific commit was made</li> </ul>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#merging-branches","title":"Merging branches","text":"<p>Master/main is (usually) considered the source of truth/most important etc etc Changes are carried out in \u201cFeature branches\u201d and commited into master when stable</p> <p>N.B. * Branches are merged. Not commits * Always merge to the current HEAD branch</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#merging-worflows-fast-forwards-commit","title":"Merging Worflows: Fast Forwards Commit","text":"<p>Workflow for simplest type of merge (a fast forward merge - where no changes have been made to the master branch, since the feature branch was created.):</p> <ul> <li>If<ul> <li>you want to merge to master</li> </ul> </li> <li>Then<ul> <li>switch to master (so HEAD points to master) and</li> <li>git merge  <p>Now * Head will point to tip of  * master and  at this point in time. * The  still exists! * From this point on further changes can be made to the feature branch and the whole process can be repeated <p>In a fast forward merge, HEAD essentially catches up with the feature branch. This assumes that no changes were made on the master branch</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#merging-worflows-merge-commit","title":"Merging Worflows: Merge commit","text":"<p>This is done (by git) when master has additional changes since the feature branch was created.</p> <ul> <li>If there are no conflicts between the new tip of master (where HEAD points to) and the , git can \u201cmerge\u201d the 2 branches. The new commit now has 2 parent commits. <li>The log for the master branch will show commits from both master and the  <p>If completely new files are added, this is not a conflict</p> <ul> <li>If there are conflcits e.g.:<ul> <li>file modified in one branch and deleted on the other</li> <li>there are changes to the same file, and at identical locations in those files.</li> </ul> </li> <li>Then<ul> <li>human needs to resolve the conflicts! </li> </ul> </li> </ul> <p>N.B. It is not necessary to merge master and a  2 sepaarte feature branches can also be merged.","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#stashing","title":"Stashing","text":"<p>Allows user to \u201cstash\u201d changes in a branch without commiting it. This is useful in scenarios where a user may need to switch to another branch where there are potentially conflicting changes. Switching to the new branch in this scenario will mean that changes in the feature branch are overwritten</p> <p>Stash puts the changes away, and reverts the changes in the working copy. It will be possible to return to those changes later.</p> <p><pre><code>git stash \n</code></pre> or <pre><code>git stash save\n</code></pre> Revert the stash by using </p> <pre><code>git stash pop\n</code></pre> <p><pre><code>git stash apply\n</code></pre> * allows for changes to be applied while not removing it from the stash. This can allow for the stash to be applied to multiple branches.</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#rebasing","title":"Rebasing:","text":"<p>To avoid cluttering git log with multiple merge messages (to keep your local repo in sync with master you will be merging main into your local repo frequently) rebase can be used</p> <p>This rewrites history, so that your feature branch has a new base at the tip of the current main branch (rather than at the point that the feature branch repo was branched)</p> <p>never rebase if your code has been shared with others (i.e. do not rebase hisotry that other have) This could result in different versions of history on different repos.. and can be very difficult/annoying for users to make sense of</p> <p>Never rebase the master branch!</p> <p><pre><code>git rebase -i HEAD \"number of commits\"\n</code></pre> * i is for interactive</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#git-workflow-to-track-changes-between-local-and-remote-repositories","title":"Git Workflow to track changes between local and remote repositories","text":"<p><pre><code>git status\n</code></pre> will show status relative to the current information in the local repo. Remote may have moved</p> <p><pre><code>git FETCH &lt;remote&gt; &lt;branchname&gt;\n</code></pre> will fetch changes on remote to the local repo but will not update your working directory</p> <p><pre><code>git status\n</code></pre> after git fetch, git status may show updated information</p> <p><pre><code>git checkout &lt;branchname&gt;\n</code></pre> to view changes (note HEAD will be detached). Working directrry will not be updated.</p> <p><pre><code>git pull\n</code></pre> to retrieve changes from remote changes will now pulled to working directory (to the working branch on the local repository)</p> <p><pre><code>git pull &lt;remote&gt; &lt;branchname&gt;\n</code></pre> is a combination of a Fetch and a Merge</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#fork-and-clone-workflow","title":"Fork and Clone workflow","text":"<ul> <li>Fork a public repo</li> <li>Clone and work on it independently<ul> <li>Cloning automatically adds your fork as the remote repository (origin)</li> </ul> </li> <li>However, you will likely want to be informed of changes in the original (upstream) repository. If so, you should add a second remote (upstream) pointing to the original repo that was forked</li> </ul> <p>To get the changes from upstream to the forked repo</p> <p><pre><code>git pull upstream main\n</code></pre> i.e. pull from upstream to main </p> <p>To request original project to incorporate changes from our feature branch:</p> <p><pre><code>git push origin main (i.e. push changes to forked repo)\n</code></pre> Then make a pull request to the upstream repo</p>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#adding-upstream-if-required","title":"Adding upstream if required","text":"<pre><code>git remote add upstream &lt;original repo url&gt;\n</code></pre>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#changing-the-remote-url","title":"Changing the remote url","text":"<p>If the remote url is moved/renamed, the change can be made locally </p> <pre><code>git remote set-url origin &lt;new url&gt;\n</code></pre>","tags":["git"]},{"location":"cheatsheets/2023-02-04-Git-Notes/#links","title":"Links","text":"<p>gitignore.io</p>","tags":["git"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/","title":"Getting Started with Jekyll","text":"<ul> <li>Step by step instructions can be found here</li> <li>Jekyll tutorial on Youtube</li> </ul>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#prerequisites","title":"Prerequisites","text":"<p>Jekyll is a Ruby gem and requires ruby and gem to be installed.</p> <pre><code>gem install jekyll bundler\n</code></pre>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#create-and-serve-a-new-jekyll-site-on-your-local-machine","title":"Create and serve a new Jekyll site on your local machine","text":"<p><pre><code>jekyll new new-site\ncd new-site\nbundle exec jekyll serve\n</code></pre> bundle exec is required when configuration changes need to be aplied.</p> <p>For changes to content the following is sufficient. <pre><code>jekyll serve\n</code></pre></p>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#managing-draft-posts","title":"Managing Draft posts","text":"<p>To serve draft posts in _drafts folder, use:</p> <pre><code>jekyll serve --draft\n</code></pre> <p>drafts are served using current date</p> <p>Use the default naming convention when a draft post is ready to be published.</p>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#permalinks","title":"Permalinks","text":"<p>By default, Jekyll uses information in categories to create url for posts.</p> <p>Custom permalinks can be crafted to override the Jekyll defaults. For example to avoid using the categories in the post url</p> <pre><code>permalink: /:year/:month/:day/:title\n</code></pre>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#front-matter-defaults","title":"Front matter defaults","text":"<p>are defined in the _config.yml file</p>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#jekyll-themes","title":"Jekyll themes","text":"<p>Theme Search</p> <p>install a theme by adding the theme to the gemfile e.g.</p> <pre><code>gem \"jekyll-theme-hacker\"\n</code></pre> <p>To install the theme, run <pre><code>bundle install\n</code></pre></p> <p>Update the config file to use the theme.</p> <p>To update the site to serve using the new theme, use</p> <pre><code>bundle exec\n</code></pre> <p>Be aware that if the new theme does not have layouts with the same names as the previous theme, pages will not render.</p>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#jekyll-variables","title":"Jekyll variables","text":"<p>List of Jekyll variables</p>","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-jekyll-notes/#deploy-to-github-pages","title":"Deploy to github pages","text":"","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-markdown-notes/","title":"Markdown","text":"","tags":["jekyll"]},{"location":"cheatsheets/2023-02-09-markdown-notes/#links","title":"Links","text":"<ul> <li>Markdown demo</li> <li>Markdown guide - Basic Syntax</li> </ul>","tags":["jekyll"]},{"location":"cheatsheets/conda/","title":"Init a repo with conda","text":"<p>Setup a new repo for local development with conda</p> <p>Workflow:</p> <ul> <li>Initiate a repo (nothing to do with conda - just makes things slightly easier)<ul> <li>create a new repo on github</li> <li>clone repo</li> <li>navigate into repo directory</li> </ul> </li> <li>create a new conda env within the dir</li> </ul> <pre><code>conda create --prefix envs  </code></pre> <ul> <li>activate environment</li> </ul> <pre><code>conda activate ./envs\n</code></pre> <ul> <li>install stuff  </li> </ul> <pre><code>conda install pip\npip install mkdocs-material\n</code></pre> <ul> <li>list conda environments</li> </ul> <pre><code>conda info --envs\n</code></pre> <ul> <li>generate an environments file </li> </ul> <pre><code>conda env export &gt; environment.yml\n</code></pre> <ul> <li>create an environment from an environment file</li> </ul> <pre><code>conda env create -f environment.yml\n</code></pre>","tags":["conda"]},{"location":"dlio_browser_tf/2023-02-16-Coursera-BrowserBasedTF-w1/","title":"Notes from Week 1 of auditing the course","text":"<p>Browsers are full fledged training / inference environments.</p> <p>Coding the \"Hello World\" example in Javascript using Tensorflow.js</p> <p>N.B. Numpy is not available in js. TF functions are used for arrays.</p> <p>Tensor2d takes as input an array as the first parameter, and the shape of the array as the second parameter</p> <p>The training code is written as an async function (As the amount of time taken for the training cannot be determined upfront, and we do not want to lock the browser when the training is going on)</p>","tags":["Coursera","Browser_Tensorflow"]},{"location":"dlio_browser_tf/2023-02-16-Coursera-BrowserBasedTF-w1/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Lab Materials</p>","tags":["Coursera","Browser_Tensorflow"]},{"location":"dlio_browser_tf/2023-02-16-Coursera-BrowserBasedTF-w1/#links","title":"Links","text":"<p>IRIS Dataset</p>","tags":["Coursera","Browser_Tensorflow"]},{"location":"dlio_browser_tf/2023-02-16-Coursera-BrowserBasedTF-w1/#running-the-example-code","title":"Running the example code","text":"<p>Instructions use a deprecated chrome extension to run a local server to execute the code</p> <p>Do the following instead,</p> <ul> <li>Open a terminal within the directory containing the code. </li> <li>python -m http.server</li> <li>Open browser and enter localhost:8000</li> <li>Click on the required file in the listing</li> </ul>","tags":["Coursera","Browser_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w1/","title":"Notes from Week 4 of auditing the course","text":"<p>In course 1, the datasets consisted of small and structured imagaes With smaller dataset (like the previous course in this specialization), there is a greater risk of overfitting.</p> <p>In this course, the focus is on training on a larger dataset (to classift cats and dogs) with images that dpecit more action and have diverse locations in the image frame.</p> <p>Data quality in public datasets is often not perfect.</p> <p>Cropping mis-classifierd images to focus on the specific feature of interest can rectify mis-classifications.</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w1/#visualizing-the-effects-of-convolutions","title":"Visualizing the effects of convolutions","text":"<p>display_grid is constructed from the variable x which is read as a feature map and processed for visibility in the central loop. This is used to render each stage i.e.  the convolutions of the image, plus their pooling, then another convolution, etc.</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w1/#visualizing-the-learning-history","title":"Visualizing the learning history","text":"<p>the \"history\" object (returned by model.fit) can be used to visualize the accuracy of training and validation.</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w1/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 1 - Lab 1</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w1/#links","title":"Links","text":"<p>Github Respository with notebooks for this course Kaggle - cats vs dogs dataset</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w1/#to-do","title":"To Do","text":"<p>Review \"Visualizing the effect of convolutions\"</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w2/","title":"Notes from Week 4 of auditing the course","text":"<p>Overfitting - model is very good at spotting features from a limited training datatset, but not so good at spotting features on data the model has not seen before.</p> <p>This can be observed by graphing the plot of training accuracy (and validation accuracy) against the epochs. If the training accuracy is high but the validation accurcay is lower and plateaues, this is a good indication of overfitting.</p> <p>Data augmentation via image transformations - mirroing/flipping, zooming, rotation, shearing, skewing, flipping increases the available training data.</p> <p>Image augmentation introduces an element of randomness in the training. If the validation set is small and does not demosntrate similar variety nd randomness, then image augmentation may not solve the overfitting issue.</p> <p>TF allows data augmentation to be done on the fly.</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w2/#image-generation","title":"Image Generation","text":"","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w2/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 2- Lab 1 Week 2 - Lab 2</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w2/#links","title":"Links","text":"<p>Preporcessing/augmentation in Keras Image data preprocessing</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-11-Coursera-CNNs-Tensorflow-w2/#to-do","title":"To Do","text":"","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w3/","title":"Notes from Week 4 of auditing the course","text":"<p>Transfer Learning - use an \"inception\" model, trained by others on larger complex datasets to augment your own model, usually with a smaller dataset. </p> <p>In practice this involves for example, \"locking\" a certian number of layer of a pre-trained model, resuing the convolutuion and pooling layers of the inception model, and adding your own DNN to it. </p> <p>This allows for models to be trained faster and also achieve higher accuracy as the inception model will have learnt many more features that will augment the model you require.</p> <p>Keras has Inception model definition built in</p> <pre><code>import os\nfrom tensorflow.keras import layers\nfrom tensorflow.kers import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nlocal_weights_file = \"\"\npre_trained_model = InceptionV3(\n# Specify the input shape for our data\ninput_shape = (150,150,3),\n# inception V3 has a fully-connected layer at the \"top\" (end?) for classification. By setting include_top to false, the fully connected layer is ignored and we go straight to the convolutions. \ninclude_top = False,\n# Do not use built in weights. \nweights = None\n)\n# load the snapshot of pre-traind weights\npre_trained_model.load_weights(local_weights_file)\n# Lock layers (so they are trainable with our code)\nfor layer in pre_trained_model.layers:\nlayer.trainable = False\n# Print summary of pre-trained model\npre_trained_model.summary()\n#Specify the last layer from the pre-trained model that we want to use. In this example, rather than the very last 3*3 convolution, we are using a previous 7*7 convolution layer (can be located by using the summary)\nlast_layer = pre_trained_model.get_layer('mixed7')\nlast_output = last_layer.output\n# Adding your own DNN\nfrom tensorflow.keras.optimizers import RMSprop\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation = 'relu')(x)\nx = layers.Dense(1, activation='sigmoid')(x)\nmodel = Model(pre_trained_model.input,x)\nmodel.compile(\noptimizer = RMSprop(learning_rate=0.0001),\nloss = 'binary_crossentropy',\nmetrics = ['acc']\n)\n</code></pre>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w3/#dropout","title":"Dropout","text":"<p>What is \"Dropout\"? It is a type of layer in Keras</p> <p>Using Dropout is a stratgey to address overfitting by remove a random number of neurons in your neural network.</p> <p>From course content: \"This works very well for two reasons: The first is that neighboring neurons often end up with similar weights, which can lead to overfitting, so dropping some out at random can remove this. The second is that often a neuron can over-weigh the input from a neuron in the previous layer, and can over specialize as a result. Thus, dropping out can break the neural network out of this potential bad habit! \"</p> <pre><code>from tensorflow.keras.optimizers import RMSprop\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation = 'relu')(x)\n# Specifies fraction of neurons to drop.\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\n</code></pre>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w3/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 3, Lab 1</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w3/#links","title":"Links","text":"<p>Imagenet Copy of pre-trained weights for Inception Keras freeze/locl layers documentation</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w3/#to-do","title":"To Do","text":"","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w4/","title":"Notes from Week 4 of auditing the course","text":"<p>Multi-class learning - moving from binary (2 class)classification problem, to 3 possible classes.</p> <p>Fashion MNIST was an example of a 10 class classification problem</p> <p>Computer graphics can be used to generate image data for training. We see this in the rock, paper, scissors classification exercise.</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w4/#key-changes-compared-to-binary-classification","title":"Key changes compared to binary classification","text":"<ul> <li>For multi-class classification, the class mode has be set to be \"categorical\"</li> <li>In the model definition - for binary classification, the output layer used a sigmoid activation fucntion which outputs a value close to 1 for one value and close to zero for the other. For multi-class classification, the the output later will need a softmax activation function. Softmax will turn the outputs in the last layer into probabilities that sum to one.</li> <li>For binary classification, when compiling the model, we used the binary cross entropy loss fucntion. For multi-class classification, we will use the categorical_cross entropy (or spare cross entropy) </li> </ul>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w4/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 4, Lab 1</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_cnn_tensorflow/2023-02-12-Coursera-CNNs-Tensorflow-w4/#links","title":"Links","text":"<p>Rock, Paper, Scissors Dataset Rock Paper Scissors Prediction Set (neither training not test/validation to simulate readl world images)</p>","tags":["Coursera","CNN_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-09-Coursera-Intro-Tensorflow-w1/","title":"Notes from auditing week 1 materials","text":"","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-09-Coursera-Intro-Tensorflow-w1/#insights","title":"Insights","text":"<p>In traditional programming, a programmer has to formulate or code rules manually, whereas, in Machine Learning, the algorithm automatically formulates the rules from the data.</p> <p>The Traditional Programming Paradigm <pre><code>(Rules, Data) -&gt; Answers\n</code></pre></p> <p>The Machine Learning Paradigm <pre><code>(Answers/Labels, Data) -&gt; Rules\n</code></pre></p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-09-Coursera-Intro-Tensorflow-w1/#coding-the-hello-world-of-deep-learning-with-neural-networks-in-tensorflow","title":"Coding the \"Hello World\" of Deep Learning with Neural Networks in Tensorflow","text":"<p>to figure out the relati  onship between x and y, given a training set with a small number of data points.</p> <pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nprint(tf.__version__)\n## Build a simple Sequential model\nmodel = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n## Compile the model\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\n## Declare model inputs and outputs for training\nxs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\nys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n## Train the model\nmodel.fit(xs, ys, epochs=500)\n## Make a prediction\nprint(model.predict([10.0]))\n</code></pre>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-09-Coursera-Intro-Tensorflow-w1/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li>Notebook - Week 1 - Lab 1</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-09-Coursera-Intro-Tensorflow-w1/#links","title":"Links","text":"<ul> <li>Lecture Notes for this course</li> <li>Tensorflow Playground</li> <li>Repo for the Tensorflow Playground</li> <li>Convnet Demo - Andrej Karpathy</li> <li>Book: Neural Networks and Deep Learning, Michael Nielson</li> <li>Book: Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/","title":"Notes from auditing week 2 materials","text":"","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#why-fashion_mnist","title":"Why Fashion_MNIST","text":"<p>\"If it doesn't work on MNIST, it won't work at all\" VS If it does work on MNIST, it may still fail on others.\"</p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#training-vs-test-splits","title":"Training vs Test splits:","text":"<p>60k images from Fashion_MNIST are used as training images and 10k as test. The test data is used to check the accuracy of a model that has been trained on the training data. The test data has not previously been seen by the model, so it is a good measure of how well the model has been trained.</p> <pre><code>model = keras.Sequential([\n# Fashion MNIST images are 28*28 grayscale. The first layer loads an image and flattens from vector to a linear array of values\nkeras.layers.Flatten(input_shape=(28,28)),\n# Hidden layer with 128 neurons. We can think of each neuron as a fucntion that takes  the 28*28 = 784 input values from the previous layer and converts it into an output value corresponding to an item in the training dataset. \nkeras.layers.Dense(128, activation=tf.nn.relu),\n# There are 10 items in the training set. Hence, the output layer has 10 neurons.\nkeras.layers.Dense(10,activation=tf.nn.softmax)\n])\n</code></pre> <p>The loss function is used to calculate how well the model did with a guess in a particular iteration. The optimzer is used to generate a new guess. Relu activation throws away negative values</p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#relu","title":"ReLU","text":"<pre><code>if x &gt; 0: \n  return x\n\nelse: \n  return 0\n</code></pre>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#softmax-intuition","title":"Softmax intuition","text":"<p>Softmax takes a list of values and scales these so the sum of all elements will be equal to 1.</p> <p>Softmax details</p> <pre><code># Declare sample inputs and convert to a tensor\ninputs = np.array([[1.0, 3.0, 4.0, 2.0]])\ninputs = tf.convert_to_tensor(inputs)\nprint(f'input to softmax function: {inputs.numpy()}')\n# Feed the inputs to a softmax activation function\noutputs = tf.keras.activations.softmax(inputs)\nprint(f'output of softmax function: {outputs.numpy()}')\n# Get the sum of all values after the softmax\nsum = tf.reduce_sum(outputs)\nprint(f'sum of outputs: {sum}')\n# Get the index with highest value\nprediction = np.argmax(outputs)\nprint(f'class with highest probability: {prediction}')\n</code></pre> <ul> <li>Input to softmax function: [[1. 3. 4. 2.]]</li> <li>In this example, these can be thought of as the values output by a 4 node NN for a classification task, with each node coressponding to a class of item. So, class 0 has value 1, class 2 has value 4 etc.</li> <li>Output of softmax function: [[0.0320586  0.23688282 0.64391426 0.08714432]]</li> <li>Sum of outputs: 1.0</li> <li>Class with highest probability: 2 (i.e the node with the value 4)</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#training-vs-testing-accuracy","title":"Training vs Testing Accuracy","text":"<p>The accuracy of training is a measure of how well the model performed after a given number of training iterations. So 0.9 means that the network is about 90% accurate in classifying the training data.</p> <p>The evaluation accuracy is a measure of how well the model performs on unseen data i.e. the test data. It can be expected that the accuracy of training will be higher that the accuracy in evaluation.</p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li>Notebook - Week 2 - Lab 1 -  Fashion MNIST workbook</li> <li>Notebook - Week 2 - Lab 2 - Using callbacks to control training</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w2/#links","title":"Links","text":"<ul> <li>Fashion MNIST</li> <li>Responsible AI practices</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/","title":"Notes from Week 3 of auditing the course","text":"","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/#why-cnns","title":"Why CNNs?","text":"<p>Introduces Convolutional Neural Networks (CNNs) as an improvement over the naive implementation of week 2. </p> <p>Pooling is way of compressing images e.g. using a 2*2 grid to pick out the pixel with the highest value to quarter the size of the image. Convolutions are essentially filters that can be passed over input data to emphasise features e.g. using fliters to highlight vertical or horizontal lines in an image.</p> <pre><code>model = keras.Sequential([\n# Adding convolution and pooling layers\n# (28,28,1) - The 1 is to indicate the single byte depth for the grascale images in FMNIST.\n# The 64 filters in the convolution layer are not random. They are a set of known good filters. See Course 4 of deep learning specialization to understand the details of convolution and pooling layers\nkeras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1))\nkeras.layers.MaxPooling2d(2,2)\nkeras.layers.Conv2D(64, (3,3), activation='relu')\nkeras.layers.MaxPooling2d(2,2)\n# End of convolution and pooling layers. Input content size has been greatly reduced.\nkeras.layers.Flatten(),\nkeras.layers.Dense(128, activation=tf.nn.relu),\nkeras.layers.Dense(10,activation=tf.nn.softmax)\n])\n</code></pre>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/#output-shape-changes-at-each-layer-due-to-the-effects-of-convolution-and-pooling","title":"Output shape changes at each layer due to the effects of convolution and pooling","text":"<p>model.summary can be used to inspect the layers of the model, including the output shapes of each layer</p> <ul> <li>Inspecting the model above will show that the output shape of the first convolution layer is (26,26,64) </li> <li>This is due to the effect of the (3,3) convolution filter -  the filter cannot be applied on the edges of the image, along a 1 pixel margin. This reduces the margin by 2 pixels on each axis </li> <li>For a (5,5) filter, the output shape will be 4 pixels less on x and y axes.</li> <li>The first 2dMax pooling halves the output along each axis and the output shape is (13,13,64)</li> <li>The second convolution layer changes the output shape to (11,11,64), due to the effect of the (3,3) filter</li> <li>The second convolution layer now reduces the output layer to (5,5,64)</li> <li>N.B For odd dimensions, max pooling will round down i.e. (11,11,64) -&gt; (5,5,64)</li> <li>The end result of convolution and pooling layers is that the 28*28 images are reduced to 5*5 images</li> <li>Remember that there are 64 convolutions. Meaning that the input to the flatten layer is 1600 pixels (rather than 784)</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/#number-of-paramters","title":"Number of paramters","text":"<p>The lecture did not walk through the number of parameters at each layer.  </p> <p>Revisit this aspect.</p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li>Notebook - Week 3 - Lab 1 - Using convolutions with FMNIST for computer vision</li> <li>Notebook - Week 3 - Lab 2 -  Exploring convolutions</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/#links","title":"Links","text":"<ul> <li>Tensorflow documentation</li> <li>Conv2D in TF</li> <li>Pooling in TF</li> <li>Convolution and Pooling details in the Deep Learning Specialization</li> <li>Lode's Computer Graphics Tutorial</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w3/#to-do","title":"To Do","text":"<ul> <li>Review the walkthrough lecture \"Improving the Fashion classifier with convolutions\"</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/","title":"Notes from Week 4 of auditing the course","text":"<p>Fashion MNIST images has extremely clean and structured images. In practice images will not be so structured. A handbag (as an example of a feature that we would like to detect) will likely not be centred in an image, but may appear anywhere in a larger picture e.g. on someone's arm</p> <p>Image Generator in TF is used to generate training and test/validation images</p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#image-generation","title":"Image Generation","text":"<pre><code>from tensorflow.keras.preprocessing.image\nimport ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nImages can be resized at runtime\ntrain_generator = train_datagen.flow_from_directory(\n# The name of the subdirectories in the training directory will be the labels for the images in that directory\ntrain_dir,\n# Images are processes as they are loaded from the directory\ntarget_size=(300,300),\n# Images are processes in batches. Batch size impact efficiency.\nbatch_size=128,\n# Specifies that this is for binary classification\nclass_mode='binary'\ntrain_generator = train_datagen.flow_from_directory(\n# The name of the subdirectories in the training directory will be the labels for the images in that directory\nvalidation_dir,\n# Images are processes as they are loaded from the directory\ntarget_size=(300,300),\n# Images are processes in batches. Batch size impact efficiency.\nbatch_size=32,\n# Specifies that this is for binary classification\nclass_mode='binary'\n)\n</code></pre>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#defining-the-model","title":"Defining the model","text":"<pre><code>model = tf.keras.models.Sequential([\ntf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\ntf.keras.layers.MaxPooling2D(2, 2),\ntf.keras.layers.Conv2D(32, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\ntf.keras.layers.Flatten(),\ntf.keras.layers.Dense(512, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\nfrom tensorflow.keras.optimizers import RMSprop\nmodel.compile(loss='binary_crossentropy',\noptimizer=RMSprop(learning_rate=0.001),\nmetrics=['accuracy'])\n</code></pre> <ul> <li>Input shape is (300, 300, 3) - The 3 indicates the three byte depth for pixels in a colour image</li> <li>The final layer has 1 output as this is a binary classifcifier (There are 2 classes and 1 value is sufficent to encode this information)</li> <li>The learning rate can be tweaked. If large flucations in accuracy are observed between epochs when training, then the learning rate should be tweaked down.</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#fitting-the-model","title":"Fitting the model","text":"<pre><code>history = model.fit(\ntrain_generator,\nsteps_per_epoch=8,  \nepochs=15,\nverbose=1)\n</code></pre> <ul> <li>The Steps per model is defined based on the batch size. 8 steps are needed in the example to load the training set in batches of size 128</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#validation-using-a-test-set","title":"Validation using a test set","text":"<ul> <li>The test of how good the model is, is determined by how it performs on images it has not previously seen. The accuracy of the model on training data is not enough.</li> <li>Accuracy on validation data can be expected to be lower than on the training data, as the model has not seen the validation data previosly.</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#the-effect-of-data-compression-on-model-accuracy","title":"The effect of data compression on model accuracy","text":"<ul> <li>Very high accuracy (~1 or &gt;1) may be noted which indicates overfitting</li> <li>The lectures highlighted that using compression may result in faster training and higher accuracy, but performance on the model may be poorer. Specifically, images that are correctly trained using a larger dataset may be misclassfied.</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li>Notebook - Week 4 - Lab 1</li> <li>Notebook - Week 4 - Lab 2</li> <li>Notebook - Week 4 - Lab 3 -  Using Compacted Images</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#links","title":"Links","text":"<ul> <li>Disease detection in Cassava plants</li> <li>Understanding different loss types</li> <li>Notes on Batch Gradient Descent</li> <li>TF RMSProp documentation</li> <li>Pixelbay - Free stock photographs</li> </ul>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_intro_tf/2023-02-10-Coursera-Intro-Tensorflow-w4/#to-do","title":"To Do","text":"<p>To do: Understand Loss vs accuracy. Are terms being used interchanegably and is this correct to do?</p>","tags":["Coursera","Intro_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w1/","title":"Notes from Week 1 of auditing the course","text":"","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w1/#preparing-and-encoding-text-for-training-in-a-neural-network","title":"Preparing and encoding text for training in a neural network","text":"<p>Encoding letters in a word in ASCII - semantics not covneyed e.g. of LISTEN vs SILENT</p> <p>Encode words/Tokenize e.g. I love my cat -&gt; [1,2,3,4] I love my dog -&gt; [1,2,3,5]</p> <pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.seuence import pad_sequences\nsentences = [\n'I love my dog'\n'I love my cat'\n'You love my dog!'\n'Do you think my dog is amazing?'\n]\n# Instruct model to encode top 100 words by volume. In this case there are fewer words, but where the number of words is unknown, this will encode the top 100 words.\ntokenizer = Tokenizer(num_words=100, oov_token=\"&lt;OOV&gt;\")\ntokenizer.fit_on_words(sentences)\n# word_index has a dictionary of unique words and their tokens (a corpus). \n# The Tokenizer strips punctuation and lowercases words.\nword_index = tokenizer.word_index\nprint(word_index)\n# Encode the sentences into lists, replacing individual words with tokens\n# If there are words the tokenizer has not seen, then these are omitted when generating sequences or with the oov_token if specified. \nsequences = tokenizer.texts_to_sequences(sentences)\nprint(sequences)\n# Sentences of different lengths need to be padded so they can be used as input into a network for training. \n# Padding is applied before the sentence by default. padding = post can be used so that padding is applied to the end rather than the start. \n# maxlen can be used to force the length of the padded sequences. This will truncate the sequences at the start. This can be overridden using the truncatin = 'post' option.\npadded = pad_sequences(sequences, padding='post', maxlen=5, truncating='post')\nprint(padded)\n</code></pre>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w1/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 1 Lab 1 Week 2 Lab 2 Week 1 Lab 3</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w1/#links","title":"Links","text":"<p>Repo for this specialization Sarcasm Dataset Sarcasm Dataset, amended for easy json loading into python</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/","title":"Notes from Week 2 of auditing the course","text":"","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#word-embeddings","title":"Word Embeddings","text":"<ul> <li>Tokens map words to a number</li> <li>Word Embeddings map tokens to vectors in higher dimensional spce</li> <li>captures semantics e.g. dog and canine may both be vectors in an n-dimensional space that are pointing in similar directions</li> </ul>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#datasets-in-tf-with-tensorflow-data-services","title":"Datasets in TF with Tensorflow Data Services","text":"<p>Use python 3 environment in Colab.</p> <pre><code>import tensorflow as tf\nprint(tf.__version__)\nimport tensorflow_datasets as tfds\n# with_info returns the metadata\nimdb, info = tfds.load(\"imdb_reviews\", with_info=True, as _supervised=True)\nimport numpy as np\n# Split the imdb dataset into its training and test components (the values in these arrays are tensors)\ntrain_data, test_data = imdb['train'], imdb['test']\n# Separate the sentences and the labels in the training_data into separate numpy arrays. Do the same for the test_data\ntraining_sentences = []\ntraining_labels = []\nfor s, l in train_data:\ntraining_sentences.append(s.numpy.decode('utf8'))\ntraining_labels.append(l.numpy()) \n# Convert the arrays of tensors into numpy arrays\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)\n</code></pre>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#tokenization-and-sequence-generation","title":"Tokenization and Sequence Generation","text":"<p>In the exmample, a vocabulory size of 10,000 is set, each sequence has a max length = 120. The details are similar to previous set of notes.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#embeddings","title":"Embeddings","text":"<p>In the example, an embedding dimension = 16 is used.</p> <p>Details of embedding are not in scope of this course. See the Coursera Deep Learning Specialization.</p> <p>Intuitively, we are defining vectors in a 16-dimension space. Words that have similar sentiment (as determined by the labels) are assigned similar vectors in this 16-dimnension space and therefore cluster together.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#observations-from-exercise-in-building-a-classifier-for-the-sarcasm-dataset","title":"Observations from exercise in building a classifier for the sarcasm dataset","text":"<p>To Do.. Accuracy vs Loss.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#loss","title":"Loss","text":"<p>Training loss falls. Validation loss increased. What does this imply?</p> <p>Number of accurate predictions increased over time. Howerver, the confidence in the predictons decreased</p> <p>Changing hyperparameters will have different impacts on accuracy and loss.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#pre-tokenised-datasets","title":"Pre tokenised datasets","text":"<p>Many datasets are available pre-tokenised. For example, the IMDB dataset is available in TFDS with sub-word tokensization</p> <p>The specific embedding used will have an impact on the layer definitions as well as the training.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 2 Lab 1 Week 2 Lab 2 Week 2 Lab 3</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-13-Coursera-NLP-w2/#links","title":"Links","text":"<p>IMDB Embedding Visualiser IMDV Reviews Dataset Tensorflow Datasets TF Subword encoder</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/","title":"Notes from Week 3 of auditing the course","text":"","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#sequence-models","title":"Sequence Models","text":"<ul> <li>Moving from sentiment in individual words to sentiment obtained from sequence of words (e.g. fun vs not fun)</li> </ul>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#rnn","title":"RNN","text":"<p>Outputs from previous stage are fed to the next stage. See details in the Deep Learning Specialization</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#lstm-long-short-term-memory","title":"LSTM (Long Short Term memory)","text":"<p>The keyword with the context comes from much earlier in the sequence. \"I grew in Ireland and when I was in school I was taight to speak ...\" In LSTM, a pipeline of \"conexts\" is fed both in the forward and reverse directions in a network.</p> <p>Is an LSTM a type of RNN?</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#lstms-in-code","title":"LSTMs in code","text":"<pre><code>model = tf.keras.Sequential([\ntf.keras.layers.Embedding(tokenizer.vocan_szie,64),\n# LSTM(64) signifies the number of outputs we desire from the LSTM layer \n# Bidirectional propogates model state in both directions (output of bidirectionlayer will be 128, rather than 64)\n# return_sequences=True is required when stacking LSTMs to ensure that output of first LSTM match the desited input of the next layer (why would this be any difefrent?)\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), return_sequences = True),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\ntf.keras.layers.Dense(64,activation='relu'),\ntf.keras.layers.Dense(1,activation='sigmoid'),\n])\n</code></pre> <p>With text/language models, the likleihood of overfitting is greater as the validation set will most likely have out of vocabulory words.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 3, Lab 1, Single layer LSTM Week 3, Lab 2, Multi layer LSTM Week 3, Lab 3, Using Convolutions Week 3, Lab 4 Week 3, Lab 5, Sracasm with LSTM Week 3, Lab 6, Sarcasm with 1D convolution</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#links","title":"Links","text":"<p>Sequence Models Course LSTM Course</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w3/#to-do","title":"To Do","text":"","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w4/","title":"Notes from Week 4 of auditing the course","text":"","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w4/#text-generation-as-a-prediction-problem","title":"Text generation as a prediction problem","text":"<p>Text generation can be thought of as as prediction problem.</p> <p>Essentially, given a sequence of text (Xs), we want to predict what is a likely sequence (Ys) that follows.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w4/#the-key-insights","title":"The key insights:","text":"<p>For every sentence in our corpus of sentences: * we can generate a number of sub-sequences on the tokenised sentences. * For every sub-sequence, all the tokens minus the last can be thought of as the input (X), and the very last token is the label(Y). This  provides our training data and training labels</p> <p>Then,for text generation, * Given an input sequence of words, we predict what the next word is * Add the predicted word back to the input sequence and used to predict the next word\u2026 ad-infinitum!   * Certainty of prediction will reduce with each prediction.</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w4/#encoding","title":"Encoding","text":"<p>The labels can be one-hot encoded so it is suitable for training.</p> <p>One-hot encoding of unique words in a corpus works well for small datasets. With larger datasets, the number of words will increase and so too will the memory requirements for one hot encoding these words.</p> <p>Character encoding can then be used - the number of characters will be less than the number of unique </p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w4/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Week 4, Lab 1 Week 4, Lab 2</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"dlio_nlp/2023-02-15-Coursera-NLP-w4/#links","title":"Links","text":"<p>Poetry Dataset TF Text generation Tutorial</p>","tags":["Coursera","NLP_Tensorflow"]},{"location":"fsdl-2021/Lecture_1/","title":"Lecture 1: Deep Learning Fundamentals","text":"<ul> <li>FSDL Page</li> <li>Video</li> <li>Slides</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#mathematical-model-of-a-neuron","title":"Mathematical model of a neuron","text":"<ul> <li>Axons: Input(s) x_i (axon_i from a neighbouring neuron)</li> <li>Synapse: Weights w_i</li> <li>Cell body: <ul> <li>sums input(s) w_ix_i and add a bias b</li> <li>Output is determined by the activation function which determines whether the neuron \"fires\" or not.</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_1/#activation-functions","title":"Activation Functions:","text":"<ul> <li>Sigmoid</li> <li>Hyperbolic Tangent</li> <li>ReLU</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#universality-of-neural-networks","title":"Universality of neural networks:","text":"<p>Hornik's Theorem - Any continuous function can be approximated with a 2-layer neural networks with enough hidden units</p> <ul> <li>Exlore interactively</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#types-of-learning","title":"Types of learning","text":"<ul> <li>Supervised  <ul> <li>Learn X given X-&gt;Y</li> </ul> </li> <li>Unsupervised<ul> <li>Learn X </li> </ul> </li> <li>Reinforcement<ul> <li>Learn to interact with an environment</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_1/#unsupervised-learning-examples","title":"Unsupervised Learning examples","text":"<ul> <li>Predict next character (charRNN - Andrej K)<ul> <li>Radford et al - 2017</li> </ul> </li> <li>Predict \"nearby\" words (word2vec)<ul> <li>Mikoloc et al - 2013</li> </ul> </li> <li>Predict next pixel (pixelCNN)<ul> <li>van den oord et al - 2016</li> </ul> </li> <li>Variational Autoencoders (VAE) - Encode image down to  latent vector/variables and then decode back - used to learn complex images from compressed representations<ul> <li>Kigma and Welling - 2014</li> </ul> </li> <li>Generative Adversarial Networks - use a latent image to generate x that is indistinguishable from real<ul> <li>Goodfellow et al - 2015</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_1/#linear-regression-line-fitting","title":"Linear Regression - line fitting","text":"<p>Can be thought of a prediction problem - Given a number of inputs X and outputs Y, what is the output corresponding to a new X we have not seen before</p> <p>Achieved by fitting a line which is in turn achieved by finding parameters w and b for a line (y = wx + b) that minimize (optimizing) the squared error loss function (min_w_b(sum(wx_i + b =y_i)^2))</p>"},{"location":"fsdl-2021/Lecture_1/#loss-functions-examples","title":"Loss functions examples","text":"<ul> <li>Mean Squared Error (MSE)</li> <li>Cross Entropy Loss</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#optimizing-loss-functions-gradient-descent","title":"Optimizing loss functions - gradient descent","text":"<p>All \"learning\" can be thought of as the problem of optimization loss functions</p> <ul> <li>choose weights randomly</li> <li>calculate the gradient (derivative) of the loss function with respect to chosen weights, using the observed measurements</li> <li>update weights by subtracting (learning rate * gradient) from current weight</li> <li>repeat</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#stochasticbatch-gradient-descent","title":"Stochastic/Batch Gradient Descent","text":"<p>Essentially computing each gradient step on subsets of the data.</p> <ul> <li>noisy</li> <li>more efficient</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#encoding-data-in-neural-networks","title":"Encoding data in neural networks","text":"<ul> <li>Computer Vision: Convolutional Neural Networks - leverage \"spatial translation invariance\" (objects look similar as we move through space)</li> <li>Natural Language Processing (sequence processing more generally): Recurrent Neural Networks - leverage \"temporal invariance\" (the rules of language do not change depending on where we are in a sentence)</li> </ul>"},{"location":"fsdl-2021/Lecture_1/#links","title":"Links","text":"<ul> <li>Neural Networks Book<ul> <li>Backpropagation</li> </ul> </li> <li>This X does not exist</li> </ul>"},{"location":"fsdl-2021/Lecture_2a/","title":"Lecture 2a: CNNs","text":"<ul> <li>FSDL page</li> <li>Video</li> <li>Slides</li> </ul>"},{"location":"fsdl-2021/Lecture_2a/#naive-implementation-of-image-classification-for-classification","title":"Naive implementation of image classification for classification","text":"<p>For a 32*32 image, with 10 possible labels, the naive approach is to have a fully connected network:</p> <ul> <li>flatten the image into a matrix with 1024 columns </li> <li>apply matrix multiplication on a 1024*10 matrix of weights to learn 10 classes.<ul> <li>(1 * 1024) * (1024 * 10) - &gt; (1 * 10)</li> </ul> </li> </ul> <p>This approach: </p> <ul> <li>scales poorly with the size of the input image - extremely inefficient<ul> <li>for a 64*64 image - 4 times as many weights as for 32* 32</li> <li>for a 128*128 image - 16 times as many weights as for 32*32</li> </ul> </li> <li>learns weights for every single pixel in the input image - overkill!<ul> <li>not all pixels contribute the key features that identify a class</li> </ul> </li> <li>will not be invariant to transformations of the input image e.g. to scaling of the input image</li> </ul>"},{"location":"fsdl-2021/Lecture_2a/#cnns","title":"CNNs","text":"<p>Convolutions of the input image apply convolution filters of a given size (say 5*5) and slide this over the image. The (5*5) output of the filter can be flattened, and matrix multiplication applied on a much smaller matrix to learn a single output coresponding to a specific filter location.</p>"},{"location":"fsdl-2021/Lecture_2a/#input-channels","title":"Input channels","text":"<p>For RGB images, the filter is applied to every channel. The size of the flattened input of the filter location will be 3 times that of the  grayscale case.</p>"},{"location":"fsdl-2021/Lecture_2a/#output-channels","title":"Output channels","text":"<p>Multiple convolutional filters can be applied, to produce multiple output channels</p>"},{"location":"fsdl-2021/Lecture_2a/#stacking-convolutional-layers","title":"Stacking convolutional layers","text":"<p>Convolutional layers can be stacked in layers. In practive each convolutional layer will be followed by an activation layer to introduce non linearity</p>"},{"location":"fsdl-2021/Lecture_2a/#strides","title":"Strides","text":"<p>Instead of sliding the filter pixel by pixel, a stride can be applied. This \"skips\" some locations ans is an approach to 'subsample' large input images</p> <p>Typically strides are the same on both x and y dimensions.</p>"},{"location":"fsdl-2021/Lecture_2a/#padding","title":"Padding","text":"<p>Padding with default values (usually 0) is applied to the edges of the image so that filters don't fall off the edges of the image.</p> <ul> <li>SAME padding - most common</li> <li>VALID padding is, confusingly, no padding </li> </ul>"},{"location":"fsdl-2021/Lecture_2a/#the-art-of-determining-the-size-of-each-output-layer-in-a-neural-network","title":"The art of determining the size of each output layer in a Neural network","text":"<p>Input: </p> <ul> <li>(W*H*D) volume</li> </ul> <p>Parameters:</p> <ul> <li>K filters, (F,F)</li> <li>Stride, (S,S)</li> <li>Padding P</li> </ul> <p>Output:</p> <ul> <li>W' = (W-F+2P)/S + 1</li> <li>H' = (H - F +2P)/S + 1</li> </ul> <p>Number of Paramters:</p> <ul> <li>Each filter has (F*F*D) parameters</li> <li>Total parameters at at layer = K*F*F*D)</li> </ul>"},{"location":"fsdl-2021/Lecture_2a/#insight","title":"Insight","text":"<p>Hyperparameters such as strides and padding are are typically not \"learnable\" as the effects of changes in padding, cannot be pre-determined - they are not differentiable and cannot be learnt in inner loop of the learning process using standard gradient descent.</p> <p>Meta learning - using an outer loop to select hyper parameters.</p>"},{"location":"fsdl-2021/Lecture_2a/#implementation-notes","title":"Implementation notes","text":""},{"location":"fsdl-2021/Lecture_2a/#receptive-fields","title":"Receptive Fields","text":"<p>The \"receptive field\" is the portion of an input image that contributed to the output of a filter.</p> <p>The receptive field can be increased by:</p> <ul> <li>stacking convolutions<ul> <li>stacking smaller sized filters typically performs better than using fewer larger filters</li> </ul> </li> <li>using dilated convolutions can be used to \"see\" a large portion the the image by skipping pixels e.g. a (3*3) filter can have a (5*5) receptive field</li> <li>decreasing size of output tensor<ul> <li>Pooling <ul> <li>sub-sampling a portion and applying an operation to reduce its size. Approaches include taking<ul> <li>Average of pixels</li> <li>Max of pixels<ul> <li>(2*2) max pooling is the most common pooling option</li> </ul> </li> </ul> </li> <li>less common to see with advances </li> </ul> </li> <li>(1*1) convolution<ul> <li>Decreases number of channels in the tensor</li> <li>Used in GoogleNet/inception</li> </ul> </li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_2a/#cnn-architecture","title":"CNN architecture","text":"<p>Notation: _n refers to the number of times the set of operations is performed</p>"},{"location":"fsdl-2021/Lecture_2a/#lenet","title":"LeNET","text":"<ul> <li>((Conv -&gt; Non Linear (ReLu/tanh))_n -&gt; Max pooling)_m -&gt; (Fully conected layers)_l -&gt; ReLU -&gt; SoftMax<ul> <li>Non linearities are typically applied between the fully connected layers</li> <li>There is typically no non-linearity after the very last fully connected layer (as loss functions expect as input, the output of a fully connected layer)</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_2a/#links","title":"Links","text":"<ul> <li>Image Kernels explained visually</li> <li>Guide to convolution arithmetic for DL</li> <li>Introduction to neural Style Transfer</li> <li>Improving how neural networks learn</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/","title":"Lecture 2b: Computer Vision Architectures","text":"<ul> <li>FSDL page</li> <li>Video</li> <li>Slides</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#imagenet","title":"Imagenet","text":"<p>Imagenet is a Large Scale Visual recognition challenge running since 2010.</p> <ul> <li>1.2M images</li> <li>1000 categories</li> <li>task - classification (but also localization and detection)</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#convnet-architectures-for-image-recognition","title":"Convnet Architectures for image recognition","text":""},{"location":"fsdl-2021/Lecture_2b/#alexnet","title":"AlexNet","text":"<p>Alexnet's performance on Imagenet challenege in 2012 using a deep neural network launched the CNN revolution Progress since then tracked by ImageNet winners</p> <p>Similar to LeNet - Introduced many innovations:</p> <ul> <li>ReLU</li> <li>Dropout</li> <li>Data augmentation</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#zfnet","title":"ZFNet","text":"<p>Very similar to AlexNet. Introduced \"deconvolutional visualizations\"</p> <ul> <li>Each filter can be thought of as detecting an image patch. Paper introuced visualisations to show how detection becomes more specific</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#vgg","title":"VGG","text":"<p>Simple deep architecture. </p> <p>Innovation:</p> <ul> <li>Stacking smaller convolutional filters to increase receptive field with fewer parameters</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#googlenetinception","title":"GoogleNet/Inception","text":"<p>As deep as VGG but with 3% parameters</p> <ul> <li>No fully connected layers</li> <li>Stack of inception modules which are different to the standard(conv-relu-conv-relu-pool) module.</li> <li>classification outputs at different portions of the network. </li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#inception-module","title":"Inception module","text":"<p>Inputs go through different filters or pooling simultaneously and the outputs get concatenated.</p>"},{"location":"fsdl-2021/Lecture_2b/#resnet","title":"ResNet","text":"<ul> <li>Very deep architecture with 152 layers</li> <li>Most commonly used now</li> <li>Lowered error rate below human performance</li> <li>Authors observed that deeper models should be able to perform as well as shallower networks of same architecture, but don't due to vanishing gradients</li> </ul> <p>Innovation:</p> <ul> <li>If a layer makes the gradient vanish, then skip around the layer by adding shortcuts </li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#resnet-variants","title":"ResNet variants","text":"<ul> <li>DenseNet<ul> <li>Multiple skip connections</li> </ul> </li> <li>ResNeXt <ul> <li>combines inception and resent</li> </ul> </li> <li>SENet<ul> <li>Adds a module of global pooling and fully connected layer to adaptively reweigh feature outputs maps</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#squeezenet","title":"SqueezeNet","text":"<p>Alexnet accuracy with 50x fewer parameters</p>"},{"location":"fsdl-2021/Lecture_2b/#comparision-of-architectures","title":"Comparision of architectures","text":"<p>DawnBecnh</p>"},{"location":"fsdl-2021/Lecture_2b/#applications","title":"Applications","text":"<ul> <li>classification<ul> <li>output the class of the image</li> </ul> </li> <li>localization<ul> <li>Show where object is, in an image</li> </ul> </li> <li>detection <ul> <li>output every object's class and locations</li> </ul> </li> <li>segmentation<ul> <li>label every pixel belonging to an object</li> </ul> </li> <li>instance segmentation<ul> <li>differentiate objects of the same class</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#classification","title":"Classification","text":""},{"location":"fsdl-2021/Lecture_2b/#localization","title":"Localization","text":"<p>Predict bounding box co-ordinates as well as class using the same network     * does not scale for detection of multiple objects</p>"},{"location":"fsdl-2021/Lecture_2b/#detection","title":"Detection","text":"<ul> <li>slide a classifier over the image at multiple scales<ul> <li>very computationally expensive</li> </ul> </li> </ul> <p>Non Maxing Suppression: When bounding boxes overlap, keep the one with the highest score.</p> <ul> <li>Overfeat 2013, used apprach of turning FC layers into convolutional layers</li> <li>YOLO/SSD ,utiple versions<ul> <li>You only look once, Single shot detection</li> <li>Multiple versions, evaluated against Microsoft CoCo common objects in context, which is a large scale object detecttion, segmentation and captioning dataset.</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#region-proposal-methods","title":"Region proposal methods","text":"<p>Look only at and classify interesting portion of an image</p> <ul> <li>Region-CNN</li> <li>Faster R-CNN</li> <li>RPN</li> <li>Mask R-CNN</li> <li>uNet / Fully convolutional nets</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#segmentation","title":"Segmentation","text":"<ul> <li>label every pixel belonging to an object</li> <li>instance segmentation<ul> <li>differentiate objects of the same class</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#mesh-rcn-predicts-3d-mesh-of-a-2d-image","title":"Mesh RCN - predicts 3D mesh of a 2D image","text":"<ul> <li>Shapenet</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#facial-landmark-detection","title":"Facial Landmark Detection","text":"<ul> <li>Annotated faces in the wild</li> <li>Papers with code</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#pose-estimation","title":"Pose estimation","text":"<ul> <li>Microsoft COCO</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#adverserial-attacks-on-cnns","title":"Adverserial attacks on CNNs","text":"<ul> <li>White box - with access to model parameters</li> <li>Black box - without access to model parameters</li> </ul> <p>Examples:</p> <ul> <li>Panda -&gt; Gibbon example - by ingroducing weel crafted \"noise\"<ul> <li>Explaining and Harnessing adverserial examples</li> </ul> </li> <li>Pictures of real world physical objects that mess up models]<ul> <li>Robust Physical-World Attacks on Deep Learning Visual Classification</li> </ul> </li> </ul> <p>Attack:</p> <ul> <li>Some form of modifying the input in the direction of the loss gradient</li> </ul> <p>Defence:</p> <ul> <li>Training with adversarial examples - not very effective in practice</li> <li>Smooth class decision boundaries: Defensive distillation - train second NN that learns to provide the same output as a nn without seeing the raw data ??</li> </ul>"},{"location":"fsdl-2021/Lecture_2b/#style-transfer","title":"Style Transfer","text":""},{"location":"fsdl-2021/Lecture_2b/#gans","title":"GANs","text":"<p>Produce life like fake images</p>"},{"location":"fsdl-2021/Lecture_2b/#links","title":"Links","text":"<ul> <li>Torchvision.models</li> <li>DawnBench</li> <li>Microsoft COCO</li> <li>ShapeNet</li> <li>Annotated Faces in the wild</li> <li>Papers with code</li> </ul>"},{"location":"fsdl-2021/Lecture_3/","title":"Lecture 3: RNN","text":"<ul> <li>FSDL page</li> <li>Video</li> <li>Slides</li> </ul> <p>RNNs are analogues of CNN for sequence data.</p>"},{"location":"fsdl-2021/Lecture_3/#sequence-problems","title":"Sequence Problems","text":""},{"location":"fsdl-2021/Lecture_3/#examples-of-sequence-problems","title":"Examples of Sequence Problems","text":"<p>Either the input, or output or both can be sequence data.</p> <ul> <li>Time series forecasting<ul> <li>Predict the next value given a time series</li> </ul> </li> <li>Sentiment classification<ul> <li>given text, determine sentiment of the text</li> </ul> </li> <li>Translation<ul> <li>given text in one language, output text in another language</li> </ul> </li> <li>Speech recognition and generation<ul> <li>given input audio, output the transcript</li> </ul> </li> <li>Text, music generation</li> <li>Captioning</li> <li>Question/Answer </li> </ul>"},{"location":"fsdl-2021/Lecture_3/#types-of-sequence-problems","title":"Types of sequence problems","text":"<p>Input - &gt; Output</p> <ul> <li>one to many<ul> <li>Input image - &gt; Output text description</li> </ul> </li> <li>many to many<ul> <li>Most general case. Where the goal is to produce an output at every step (for every input in the sequence)</li> </ul> </li> <li>many to one<ul> <li>use last output of the many to many in RNN</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_3/#why-not-use-feed-forward-networks","title":"Why not use feed-forward networks?","text":"<ul> <li>Does not handle sequences of arbitrary length well<ul> <li>could pad sequence to max length allowed</li> <li>memory requirement will scale linearly in the number of timesteps - undesirable scaling property</li> </ul> </li> <li>Would need to learn patterns everywhere they may occur in a sequence. This ignores the temporal nature of the problem.</li> </ul>"},{"location":"fsdl-2021/Lecture_3/#sequence-architectures","title":"Sequence Architectures","text":""},{"location":"fsdl-2021/Lecture_3/#rnns","title":"RNNs","text":"<ul> <li>Core idea of RNNs: Stateful computations<ul> <li>o/p depends on input at current time step as well as the (hidden) state from previous step(s)</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_3/#encoderdecoder-architecture","title":"Encoder/Decoder Architecture","text":"<ul> <li>Encoder maps input to a vector/hidden state</li> <li>Decoder maps vector to output* Not parallelizable - slow in practice</li> <li> <p>Flexible architecture that works for all types of sequence problems.</p> <ul> <li> <p>Many to one:</p> <ul> <li>Encoder: RNN for text input</li> <li>Decoder: Fully Connected</li> </ul> </li> <li> <p>One to many:</p> <ul> <li>Encoder: CNN for image input</li> <li>Decoder: RNN for text output</li> </ul> </li> <li> <p>Many to Many:</p> <ul> <li>Encoder: RNN for character input sequences</li> <li>Decoder: RNN for character input sequences</li> </ul> </li> </ul> </li> </ul> <p>Thinking in terms of encoder and decoder architectures is a useful mental model only. In practice, they are not separate in any meaningful sense. The back-propagation propagates through the decoder and encoder portions of the network</p>"},{"location":"fsdl-2021/Lecture_3/#problems-with-vanilla-rnn-architectures","title":"Problems with vanilla RNN architectures","text":"<ul> <li>All information in input sequence (regardless of its size) is condensed to a single hidden state vector of a pre-determined dimension, on which the decoder operates.</li> <li>Vanishing gradients:<ul> <li>activation function such as sigmoid and tanh have derivatives &lt;&lt;1 near saturation. At each step, the magnitude of the gradients tends to decrease and after enough steps, the gradients could be so small that  they have \"vanished\"</li> </ul> </li> <li>Exploding gradients:<ul> <li>problem with ReLU </li> </ul> </li> </ul> <p>To do : Review Back-propagation through time - unroll graph before performing back propogation steps</p>"},{"location":"fsdl-2021/Lecture_3/#lstm-architecture","title":"LSTM architecture","text":"<p>The architectures used in practice to get around the problems associated with vanishing gradients. This is achieved by using a \"cell state\" that is propagated through the network.</p> <ul> <li>Forget gate:<ul> <li>decide which parts of old state to forget</li> </ul> </li> <li>Input gate:<ul> <li>decide which new information to introduce into cell state</li> </ul> </li> <li>Output gate:<ul> <li>produce the hidden state to be used at the next time step </li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_3/#comparison-of-architectures-studies-may-appear-inconclusive","title":"Comparison of architectures - Studies may appear inconclusive","text":"<ul> <li>Literature comparing sequence model architectures<ul> <li>[A Search Space Odyssey, Gref er al.]<ul> <li>Compared 9 variants, using 3 datasets</li> <li>Cannot beat LSTM</li> </ul> </li> <li>[An empirical Exploration of RNN architectures, Josefowicz, Zaremna, Sutskever]<ul> <li>compared 10k architectures!</li> <li>GRU is the best</li> </ul> </li> </ul> </li> </ul> <p>Conclusion (FSDL):</p> <ul> <li>LSTM work well for most tasks</li> <li>Try GRU if LSTMs dont perform as well as expected</li> </ul>"},{"location":"fsdl-2021/Lecture_3/#lstm-problems-and-some-innovation-in-google-machine-translation-approach-2016","title":"LSTM problems and some innovation in Google Machine Translation approach (2016)","text":"<ul> <li>Stacked LSTMs to improve learning</li> <li>Hard to train for very deep (&gt;4) stacks of LSTMs<ul> <li>Sol: Skip connections</li> </ul> </li> <li>Lots of information stacked in last time step<ul> <li>Sol: Attention: Give neural network access to the entire history to enable longer-term connections</li> <li>For machine translation, relevance scores assigned to individual words in the input sequence Also has applications in audio (e.g transcription) and image (e.g. captioning)</li> </ul> </li> <li>Only consider backward context<ul> <li>Understanding how a sentence unfolds can depend on information that appears in the future.</li> <li>Sol: Bi-directionality: Use one LSTM to process the sequence in the forward order and another in the backward order.</li> </ul> </li> </ul>"},{"location":"fsdl-2021/Lecture_3/#ctc-loss-algorithm","title":"CTC Loss algorithm","text":"<p>When attempting to convert (decipher :) handwritten text in image format, a convnet + LSTM architecture can be used to convert segments of the image to text. However, this approach is sensitive to common issues such as the scaling if the input image, which may result in repeated characters.</p> <p>Sol: Combines repeating characters, with tokens to cater for expected repeating characters</p> <ul> <li>Connectionist Temporal Classification</li> </ul>"},{"location":"fsdl-2021/Lecture_3/#non-recurrent-sequence-algorithms","title":"Non recurrent sequence algorithms","text":"<ul> <li>Wavenet </li> <li>uses convolutions  </li> <li>Wavenet (trained on internal Google datasets) </li> <li>uses a window of hidden layers, going all the way down to the input data</li> <li>1d causal convolutions - windows of inputs only in past</li> <li>dilated convolutions to increase the receptive field (to look at wider array of inputs)</li> <li>inherrently parallel in training</li> <li>slow inference time - mitigated with parallel wavenet</li> <li>Transformer architecture</li> <li>Since 2019, almost all sequence algorithms have been replaced by the Transformer architecture</li> </ul>"},{"location":"fsdl-2021/Lecture_3/#links","title":"Links","text":"<ul> <li>The Unreasonable Effectiveness of Recurrent Neural Networks</li> <li>When Recurrent Models Don't Need to be Recurrent</li> <li>Attentional-Interfaces</li> <li>Visualising attention.<ul> <li>Attentional-Interfaces</li> <li>Attention-Vis</li> </ul> </li> <li>Connectionist Temporal Classification</li> <li>Attention Craving RNNs: Building Up To Transformer Networks</li> </ul>"},{"location":"fsdl-2021/Lecture_4/","title":"Lecture 4:","text":"<ul> <li>FSDL page</li> <li>Video</li> <li>Slides</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/","title":"Long list of references, courses...","text":"<p>This is a summary of references from a public Discord chat by Charles F</p> <p>Link to FSDL Discord</p>"},{"location":"fsdl-2022/2023-02-22-References/#fsdl-course-prep-references-from-the-courses-discord-server-course-prep-channel","title":"FSDL Course Prep References (from the course's Discord server, -course-prep channel)","text":""},{"location":"fsdl-2022/2023-02-22-References/#deep-learning-foundations","title":"Deep Learning Foundations","text":"<ul> <li>Andy NG courses</li> <li>Google ML crash course</li> <li>Alfredo Canziani's NYU course with Yann Le Cun</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#software-engineering","title":"Software Engineering","text":"<ul> <li>\"The Missing Semester of Your CS Education\" from MIT</li> <li>SWE for AI - by Lightning.AI</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#math-fundamentals-linear-algebra-calculus-probability","title":"Math fundamentals -- linear algebra, calculus, probability.","text":"<ul> <li>Charles F, YouTube series on Math4ML, Programmer's view of math for ML.</li> <li>3Blue1Brown's Essence of Linear Algebra </li> <li>3Blue1Brown's Essence of Calculus</li> <li>Jeremy Kun's Programmer's Introduction to Mathematics</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#course-tech-stack-for-2022","title":"Course Tech Stack for 2022","text":"<ul> <li>Deep Learning : PyTorch</li> <li>PyTorch Lightning</li> <li>torchmetrics</li> <li>MLOps: Weights &amp; Biases</li> <li>(https://www.youtube.com/watch?v=G7GH0SeNBMA)</li> <li>Deployment: Docker, * AWS Lambda</li> <li>AWS EC2</li> <li>Frontend: Gradio</li> <li>Jupyter Notebooks</li> <li>Colab </li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#good-threardsdiscussions","title":"Good threards/discussions","text":"<ul> <li>Advice from Charles F to avoid \"Dropping off\"</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#other-learning-resources","title":"Other learning resources","text":"<ul> <li>Computer science</li> <li>More CS</li> <li>Javascript</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#links-from-other-bits-of-the-discord-server","title":"Links from other bits of the Discord server","text":"<ul> <li>Do not feed the Energy Vampire</li> </ul>"},{"location":"fsdl-2022/2023-02-22-References/#reading-groups","title":"Reading groups","text":"<ul> <li>Reading group for discussing paper relevant to making ML work in production</li> <li>Shreya Shankar et al.'s \"Operationalizing ML\" interview study. </li> </ul>"},{"location":"fsdl-2022/labs/Links-2022-Labs/","title":"Links 2022 Labs","text":"Lab Colab Video Lab 01: Deep Neural Networks in PyTorch Lab 02a: PyTorch Lightning Lab 02b: Training a CNN on Synthetic Handwriting Data Lab 03: Transformers and Paragraphs Lab 04: Experiment Management Lab 05: Troubleshooting &amp; Testing Lab 06: Data Annotation Lab 07: Web Deployment Lab 08: Model Moinitoring"},{"location":"fsdl-2022/labs/fsdl-2022-lab-0-overview/","title":"The FSDL App","text":"<ul> <li>Lab Overview - FSDL</li> <li>Setup instructions</li> <li>Lab Overview - Colab</li> <li>The deployed FSDL text recognizer application</li> <li>repo for the FSDL text recognizer application</li> <li>repo for the labs</li> </ul>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-0-overview/#tech-stack","title":"Tech Stack:","text":"<ul> <li>Model Serving <ul> <li>Ingress as a service</li> <li>Frontend Library - Gradio</li> <li>Model is wrapped in a Docker container</li> <li>Model is deployed serverlessly using AWS Lambda</li> <li>The container image lives in Amazon's Elastic Container Registry</li> </ul> </li> <li>Monitoring<ul> <li>Monitoring Model Performance</li> </ul> </li> <li>Model Training<ul> <li>Lambda Labs GPU</li> <li>The Pytorch Library provides GPU accelerated array math and fucntionality for model training</li> <li>The Pytorch Lightning Library provides the framework for training models using Pytorch.</li> </ul> </li> </ul>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-0-overview/#tutorials","title":"Tutorials:","text":"<ul> <li>Docker Tutorial</li> <li>What is serverless</li> <li>Convert Pytorch Models to Pytorch Lightning </li> </ul>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-0-overview/#experiment-and-artefact-tracking","title":"Experiment and artefact Tracking","text":"<ul> <li>Weights and Biases stores</li> <li>PyTorch lighting checkpoits that can be used to restart model training if interrupted. This enables used of chepaer preemptible cloud instance. These are essentially excess capacity that a cloud provider has which can be utilised, with the caveat that resources can be pre-empted (allocated away at the cloud provider's will)</li> <li>deployed model files</li> </ul>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-0-overview/#model-handoff-to-production","title":"Model Handoff to production","text":"<ul> <li>Pytorch lightning models are compiled down to a dialect of Torch called torchscript</li> </ul>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-1-3-pre-requisites/","title":"FSDL-2022-Labs-1-3 pre labs","text":"Lab Colab Video Lab 01: Deep Neural Networks in PyTorch Lab 02a: PyTorch Lightning Lab 02b: Training a CNN on Synthetic Handwriting Data Lab 03: Transformers and Paragraphs"},{"location":"fsdl-2022/labs/fsdl-2022-lab-1-3-pre-requisites/#lab-01-deep-neural-networks-in-pytorch","title":"Lab 01: Deep Neural Networks in PyTorch","text":"<p>Within Jupyter, peek into the code of a module by using ?? e.g. <pre><code>import text_recognizer.data.util as util\nutil??\n</code></pre> <ul> <li>\"Tensor Puzzles\" by Sasha Rush</li> <li>What Tensor Puzzles reveal?</li> <li>Pytorch Internals</li> </ul> <p>This Lab builds a model from scratch using Python/Pytorch.</p>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-1-3-pre-requisites/#lab-02a-pytorch-lightning","title":"Lab 02a: Pytorch Lightning","text":"<p>Brown Corpus</p> <p>Lightning can be thought of a way to organise Pytorch code. See here</p>"},{"location":"fsdl-2022/labs/fsdl-2022-lab-1-3-pre-requisites/#lab-02b-training-a-cnn-on-synthetic-handwriting-data","title":"Lab 02b: Training a CNN on Synthetic Handwriting Data","text":""},{"location":"fsdl-2022/labs/fsdl-2022-lab-1-3-pre-requisites/#lab-03-transformers-and-paragraphs","title":"Lab 03: Transformers and Paragraphs","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-1/","title":"Course overview","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-1/#the-commoditization-of-model-training","title":"The commoditization of model training","text":"<ul> <li>Deploy state of the art language and vision models with mnimum code<ul> <li>Huggingface</li> </ul> </li> <li>Model as a service<ul> <li>OpenAI</li> </ul> </li> <li>Frameworks for models<ul> <li>Keras</li> <li>PyTorch Lightning</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-1/#progress-of-ai","title":"Progress of AI","text":"<ul> <li>5 Things you should know about AI - 2017 article</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-1/#when-to-use-ml","title":"When to use ML","text":"<ul> <li>\"...their value must outweigh not just the cost of developing them but also the additional complexity that these ML systems introduce to your software<ul> <li>\"The High-Interest Credit Card of Technical Debt\"</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-1/#finding-high-impact-ml-project","title":"Finding high impact ML project","text":"<ul> <li>Projects which will make predictions that are central to decision making cheaper.<ul> <li>\"Prediction Machines: The Simple Economics of AI.\"</li> </ul> </li> <li>What a product needs<ul> <li>Principles of Spotify's ML powered \"Discover Weekly\" feature</li> </ul> </li> <li>Parts of a system that is complex andd manually defined<ul> <li>Andrej Karpathy's idea of Software 2.0</li> <li>Github Copilot</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-1/#mlops-at-scale","title":"MLOPS at scale","text":"<ul> <li>Jacopo Tagliabue talk on MLOps at Reasonable Scale</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/","title":"Development Infrastructure and Tooling","text":"<ul> <li>nbdev - Writing tests on Jupyter notebooks</li> <li>streamlit - Building Python web apps interactively</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#frameworks-for-mlai","title":"Frameworks for ML/AI","text":"<ul> <li>Pytorch<ul> <li>The dominance of Pytorch (as of 2022)<ul> <li>https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/</li> <li>https://blog.mlcontests.com/p/winning-at-competitive-ml-in-2022?s=w</li> </ul> </li> <li>Pytorch Lightning </li> </ul> </li> <li>Tensorflow</li> <li>Jax<ul> <li>Deep Learning Libraries for JAX</li> <li>Haiku</li> <li>Jax</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#meta-frameworks-and-model-zoos","title":"Meta Frameworks and Model zoos","text":"<ul> <li>Onyx - Open standard for deep learning models</li> <li>Hugging Face</li> <li>TIMM - collection of CV models</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#distributed-training","title":"Distributed Training","text":"<p>There are different scenarios depending on  * whether data fits on a single GPU or not * the model parameters fit on a single GPU or not</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#when-model-fits-on-single-gpu","title":"When model fits on single GPU","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#data-parallelism","title":"Data parallelism","text":"<ul> <li>Using the Pytorch Distributed Data Parallel Library </li> <li>Horovod</li> </ul> <p>https://www.reddit.com/r/MachineLearning/comments/hmgr9g/d_pytorch_distributeddataparallel_and_horovod/</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#when-the-model-does-not-fit-on-a-single-gpu","title":"When the model does not fit on a single GPU","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#sharded-data-parallelism","title":"Sharded Data Parallelism","text":"<p>What takes up GPU memory?</p> <ul> <li>The model parameters that make up model layers</li> <li>The gradients needed to do back-propagation.</li> <li>The optimizer states include statistics about the gradients</li> <li>and, a batch of data for model development.</li> </ul> <p>Sharded data parallelism shards model parametersm gradients and optimzer states, meaning that large batch sizes can be used. Examples are</p> <ul> <li>Microsoft Zero</li> <li>https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</li> </ul> <p>Shared data parallelism is mplemented by: * Microsoft Deepspeed * Facebook Fairscale   * Fairscale CPU offloading - Zero principle on a single GPU * Pytorch Fully-sharded data parallel</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#pipelined-model-parallelism","title":"Pipelined Model Parallelism","text":"<p>This approach puts each layer on each GPU. However, this approach means that only one GPU is active at any given time.</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#tensor-parallelism","title":"Tensor-parallelism","text":"<p>This approach distributes matrix multiplications across multiple GPUs.</p> <ul> <li>NVidia Megatron-LM repo for the tranformer model</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#combine-all-3-options","title":"Combine all 3 options","text":"<ul> <li>Bloom</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#resources-to-speed-up-model-training","title":"Resources to speed up model training","text":"<ul> <li>Deepspeed</li> <li>MosaicML</li> <li>FFCV</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#compute","title":"Compute","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#gpu-benchmarks","title":"GPU benchmarks","text":"<ul> <li>Lambda Labs GPU benchmarks</li> <li>AIME</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#startups-that-provide-gpu-access","title":"Startups that provide GPU access","text":"<ul> <li>Paperspace</li> <li>Coreweave</li> <li>Lambda Labs</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#comparing-the-cost-of-gpu-access","title":"Comparing the cost of GPU access","text":"<p>FSDL tool</p> <p>Insight: \"the most expensive per hour chips are not the most expensive per experiment!\"</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#resource-management","title":"Resource Management","text":"<ul> <li>SLURM workload management</li> <li>Managing ML projects with Docker + Kubernetes -&gt; Kubeflow</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#end-to-end-solutions-for-clustercompute-management","title":"End to end solutions for cluster/compute management","text":"<ul> <li>AWS Sagemaker</li> <li>Anyscale</li> <li>Ray</li> <li>Ray Train</li> <li>Grid.ai</li> <li>Uncertain future?</li> <li>Determined.ai</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#experiment-and-model-management","title":"Experiment and model management","text":"<ul> <li>TensorBoard</li> <li>MLFlow</li> <li>Weights and Biases</li> <li>Hyperparameter Sweeping</li> <li>Nepture AI</li> <li>Comet ML</li> <li>Determined AI</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-2/#all-in-one-solutions","title":"All in One solutions","text":"<ul> <li>Graident</li> <li>Domino Data Lab</li> <li>AWS Sagemaker</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-3/","title":"Troubleshooting and Testing","text":"<ul> <li>Test Suite ~ Classifiers comparision</li> <li>Pytest</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-3/#writing-clearn-code","title":"Writing clearn code","text":"<ul> <li>The Python Formatter - Black</li> <li>Flake8</li> <li>Shellcheck</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-3/#testing-automation","title":"Testing Automation","text":"<ul> <li>Github Actions</li> <li>Pre commit CI</li> <li>Circle CI</li> <li>Jenkins</li> <li>precommit</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-3/#testing-ml-systems","title":"Testing ML systems","text":"<ul> <li>Expectation testing on data with Great Expectations</li> <li>Tutorial</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-3/#testing-nlp","title":"Testing NLP","text":"<ul> <li>Checklist</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-3/#a-model-for-trobleshooting","title":"A model for Trobleshooting","text":"<ul> <li>Make it run</li> <li>Make it fast</li> <li>Make it right</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/","title":"Data Management","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#most-ml-tools-are-databses-at-their-core","title":"Most ML Tools are databses at their core","text":"<ul> <li>Weights and Biases is a database of experiments</li> <li>Hugging Face is a database of models</li> <li>Label Studio is a database of labels</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#platforms-unifying-structured-and-unstructured-data","title":"Platforms unifying structured and unstructured data","text":"<ul> <li>Book exploring topic from first principles</li> <li>Snowflake</li> <li>Databricks</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#data-exploration","title":"Data Exploration","text":"<ul> <li>SQL</li> <li>Dataframe</li> <li>Pandas<ul> <li>DASK - Parallelise Pandas Operations over cores</li> <li>RAPIDS - Pandas operations om GPUs</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#data-processing","title":"Data Processing","text":"<p>Managing task dependencies/run models on schedule using a Directed Acyclic Graph workflow of data operations</p> <ul> <li>Airflow</li> <li>Prefect</li> <li>Dagster</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#feature-stores","title":"Feature stores","text":"<ul> <li>Uber - Michelangelo</li> <li>Tecton</li> <li>Feast</li> <li>Featureform</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#datasets","title":"Datasets","text":"<ul> <li>Huggingface datasets</li> <li>Activeloop</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#data-labelling","title":"Data Labelling","text":"<ul> <li>Self supervised learning - models can have elements of data masked and the model can use earlier parts of the data to preidct masked parts<ul> <li>OpenAI-CLIP</li> </ul> </li> <li>Image data augmentation<ul> <li>Torch Vision</li> <li>SimCLR</li> </ul> </li> <li>Synthetic data</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#labelling-solutions","title":"Labelling Solutions","text":"<ul> <li> <p>Crowdsourced:</p> <ul> <li>Amazon Mechanical Turk</li> </ul> </li> <li> <p>Full service:</p> <ul> <li>Scale</li> <li>Labelbox</li> <li>Supervisely</li> <li>Labelstudio</li> <li>Diffgram</li> <li>AquariumLearning</li> <li>Scale-Nucleus</li> <li>Snorkel</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-4/#data-versioning","title":"Data versioning","text":"<ul> <li>DVC</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/","title":"Deployment","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#build-a-prototype-separate-your-model-and-ui","title":"Build a prototype / Separate your model and UI","text":"<ul> <li>Huggingface</li> <li>Gradio</li> <li>Streamlit</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#separate-your-model-and-ui","title":"Separate your model and UI","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#batch-prediction-run-model-on-each-data-point-and-store-results-in-a-database","title":"Batch Prediction - Run model on each data point and store results in a database","text":"<ul> <li>See Data processing - Lecture 4</li> <li>Metaflow</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#model-as-a-service-run-the-model-as-a-separate-service","title":"Model as a service - Run the model as a separate service","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#api","title":"API","text":"<ul> <li>REST</li> <li>GRPC</li> <li>GraphQL</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#learn-the-tricks-to-scale","title":"Learn the tricks to scale","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#consider-moving-your-model-to-the-edge-when-you-really-need-to-go-fast","title":"Consider moving your model to the edge when you really need to go fast","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#dependency-management","title":"Dependency Management","text":"<ul> <li>Docker<ul> <li>Cog</li> <li>BentoML</li> <li>Truss</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#monitoring-performace","title":"Monitoring Performace","text":"<ul> <li>Example FSDL Project<ul> <li>Inference - Triton</li> <li>Monitoring - Prometheus</li> <li>Analytics - Grafana</li> </ul> </li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#performance-optimization","title":"Performance Optimization","text":"<p>Revisit later</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#edge-deployment","title":"Edge deployment","text":"<ul> <li>Nvidia - TensorRT </li> <li>Android - MLKit</li> <li>iOS - CoreML</li> <li>ios and Android/Python</li> <li>Tensoflow - TFLite</li> <li>Browser - Tensorflow JS</li> <li>Target device agnostic - Apache TVM</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-5/#startups","title":"Startups:","text":"<ul> <li>MLIR</li> <li>OctoML</li> <li>TinyML</li> <li>Modular</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-6/","title":"Continual Learning","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-6/#monitoring-tools","title":"Monitoring Tools","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-6/#system-monitoring","title":"System Monitoring","text":"<ul> <li>Datadog</li> <li>Honeycomb</li> <li>New Relic</li> <li>Amazon Cloudwatch</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-6/#ml-specific-monitoring","title":"ML Specific Monitoring","text":"<ul> <li>Evidently AI</li> <li>whylogs</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-6/#saas-based-ml-monitoring-and-observability","title":"SaaS based Ml Monitoring and observability","text":"<ul> <li>Gantry</li> <li>Aporia</li> <li>Superwise</li> <li>Arize</li> <li>Fiddler</li> <li>Arthur</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-6/#data-curation","title":"Data Curation","text":"<p>Encriching streams of data into forms suitable for training</p> <ul> <li>Scale Nucleus</li> <li>Aquarium</li> <li>Gantry</li> </ul>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-7/","title":"Foundation Models","text":"<p>This lecture is the highlight of the course.</p>"},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-8/","title":"ML Teams and Project Management","text":""},{"location":"fsdl-2022/lectures/fsdl-2022-lecture-9/","title":"Ethics","text":""},{"location":"karpathy/","title":"Index","text":"<ul> <li>Andrej Karpathy's - Neural Networks: Zero to Hero</li> </ul>"},{"location":"karpathy/micrograd-1/","title":"Spelled out Introduction to Neural Networks","text":"<p>On Youtube</p>"},{"location":"karpathy/nanoGPT-1/","title":"Notes from Andrej Karpathy's NanoGPT codealong","text":"<ul> <li>ChatGPT</li> <li>generates text - left to right</li> <li>is a probabilistic system</li> <li>stands for Chat \"Generatively Pre-trained transformer\"</li> <li>is a language model - it models the sequence of characters or words or token. It predicts how charactes/words/tokens follow each other in a language</li> <li>given a question/prompt, ChatGPT is completing the sequence.</li> <li> <p>is based on the transformer architecture (see the 2017 landmark paper,  attention is all you need)</p> </li> <li> <p>NanoGPT</p> </li> <li>trained on [OpenWebtext]</li> <li>reproduces GPT2 124 Million parameter model</li> </ul> <p>Codealong: ~NanoGPT   * is a character level language model   * trained on Tiny Shakespeare   * generates infinite Shakespeare</p>"},{"location":"karpathy/nanoGPT-1/#tokenization","title":"Tokenization","text":"<ul> <li>character level</li> <li>used in the codealong</li> <li>word level</li> <li>sub-word level</li> <li>Google Sentence piece</li> <li>OpenAI tiktoken (used in GPT)</li> </ul> <p>tradeoff codebook size and sequence lengths: with word level or sub word level tokenization, there are a larger number of tokens, but this results in much more compact encoding.</p>"},{"location":"karpathy/nanoGPT-1/#training","title":"Training","text":"<ul> <li>Training happens on: </li> <li>on chunks of data of given blocksize (or context length)<ul> <li>In each block, there are 'blocksize' number of individual \"contexts\"</li> <li>the blocksize gives us the 'time' component (?)</li> </ul> </li> <li>on chucks of given batchsize. <ul> <li>Chunks are trained independently</li> </ul> </li> </ul>"},{"location":"karpathy/nanoGPT-1/#bigram-language-model","title":"Bigram Language Model","text":"<ul> <li>See makemore series</li> </ul>"},{"location":"karpathy/nanoGPT-1/#jupyter-notebooks","title":"Jupyter Notebooks","text":""},{"location":"karpathy/nanoGPT-1/#links","title":"Links","text":"<ul> <li>Andrej Karpathy's site</li> <li>Karpathy - Makemore Tutorial Playlist</li> </ul>"},{"location":"karpathy/nanoGPT-1/#links-specific-to-codealong","title":"Links (specific to codealong)","text":"<ul> <li>ChatGPT Prompt/Response Library</li> <li>Attention is all you need</li> <li>Tiny Shakespeare</li> <li>nanoGPT</li> <li>OpenWebText</li> <li>GPT 2 weights released by OpenAI</li> <li>Sentence piece</li> <li>Tiktoken</li> </ul>"},{"location":"karpathy/nanoGPT-1/#to-do","title":"To Do","text":"<ul> <li>walkthrough the makemore playlist</li> <li>revist this lecture from timestamp 25</li> </ul>"}]}